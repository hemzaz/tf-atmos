---
# =============================================================================
# DATA PIPELINE STACK TEMPLATE
# =============================================================================
# Complete data processing pipeline infrastructure
#
# Architecture:
#   - Kinesis Data Streams for real-time ingestion
#   - Lambda functions for data transformation
#   - Kinesis Firehose for delivery to S3 (Data Lake)
#   - Glue Catalog and Crawlers for schema management
#   - Athena for ad-hoc SQL queries
#   - QuickSight for visualization (manual setup required)
#   - EventBridge for pipeline orchestration
#   - Step Functions for complex ETL workflows
#   - S3 buckets with lifecycle policies
#
# Cost Estimates (Monthly):
#   Development:  ~$100-250/month
#   Staging:      ~$300-600/month
#   Production:   ~$800-5,000/month (depending on data volume)
#
# Deployment Time: 20-35 minutes
#
# Usage:
#   atmos terraform plan data-pipeline -s <tenant>-<account>-<environment>
#   atmos terraform apply data-pipeline -s <tenant>-<account>-<environment>
# =============================================================================

name: data-pipeline
description: "Complete data processing pipeline with Kinesis, Lambda, Glue, and Athena"
version: "1.0.0"

# Import base configurations
import:
  - catalog/_base/defaults
  - catalog/iam/defaults
  - catalog/monitoring/defaults

# =============================================================================
# COMPONENT DEFINITIONS
# =============================================================================

components:
  terraform:
    # =========================================================================
    # DATA LAKE STORAGE (S3)
    # =========================================================================

    data-pipeline/s3-raw:
      metadata:
        component: s3
        type: real
      vars:
        bucket_name: "${tenant}-${environment}-data-lake-raw"
        description: "Raw data landing zone"

        # Versioning
        versioning_enabled: true

        # Encryption
        server_side_encryption:
          sse_algorithm: "aws:kms"
          kms_master_key_id: "${data_lake_kms_key_id}"

        # Lifecycle rules
        lifecycle_rules:
          - id: "transition-to-ia"
            enabled: true
            prefix: ""
            transitions:
              - days: 30
                storage_class: "STANDARD_IA"
              - days: 90
                storage_class: "GLACIER"
            expiration:
              days: 365
            noncurrent_version_transitions:
              - days: 30
                storage_class: "GLACIER"
            noncurrent_version_expiration:
              days: 90

        # Access control
        block_public_access: true
        block_public_acls: true
        block_public_policy: true
        ignore_public_acls: true
        restrict_public_buckets: true

        # Logging
        logging:
          target_bucket: "${log_bucket_name}"
          target_prefix: "s3-access-logs/raw/"

        # Replication (for DR)
        replication_configuration:
          enabled: "${enable_cross_region_replication | default(false)}"
          destination_bucket: "${dr_bucket_arn}"
          destination_region: "${dr_region}"

      tags:
        DataClassification: "raw"
        DataPipeline: "ingestion"

    data-pipeline/s3-processed:
      metadata:
        component: s3
        type: real
      vars:
        bucket_name: "${tenant}-${environment}-data-lake-processed"
        description: "Processed and transformed data"

        # Versioning
        versioning_enabled: true

        # Encryption
        server_side_encryption:
          sse_algorithm: "aws:kms"
          kms_master_key_id: "${data_lake_kms_key_id}"

        # Lifecycle rules (keep processed data longer)
        lifecycle_rules:
          - id: "transition-to-ia"
            enabled: true
            prefix: ""
            transitions:
              - days: 90
                storage_class: "STANDARD_IA"
              - days: 365
                storage_class: "GLACIER"
            noncurrent_version_expiration:
              days: 180

        # Access control
        block_public_access: true

        # Logging
        logging:
          target_bucket: "${log_bucket_name}"
          target_prefix: "s3-access-logs/processed/"

      tags:
        DataClassification: "processed"
        DataPipeline: "transformation"

    data-pipeline/s3-curated:
      metadata:
        component: s3
        type: real
      vars:
        bucket_name: "${tenant}-${environment}-data-lake-curated"
        description: "Curated data ready for analytics"

        # Versioning
        versioning_enabled: true

        # Encryption
        server_side_encryption:
          sse_algorithm: "aws:kms"
          kms_master_key_id: "${data_lake_kms_key_id}"

        # Lifecycle rules (analytics data retained longer)
        lifecycle_rules:
          - id: "transition-to-ia"
            enabled: true
            prefix: ""
            transitions:
              - days: 180
                storage_class: "STANDARD_IA"
            noncurrent_version_expiration:
              days: 365

        # Access control
        block_public_access: true

        # Logging
        logging:
          target_bucket: "${log_bucket_name}"
          target_prefix: "s3-access-logs/curated/"

        # S3 Inventory for data catalog
        inventory_configuration:
          enabled: true
          destination_bucket: "${tenant}-${environment}-data-lake-inventory"
          format: "Parquet"
          frequency: "Weekly"
          included_object_versions: "Current"
          optional_fields:
            - "Size"
            - "LastModifiedDate"
            - "StorageClass"
            - "ETag"

      tags:
        DataClassification: "curated"
        DataPipeline: "analytics"

    # =========================================================================
    # DATA INGESTION (Kinesis)
    # =========================================================================

    data-pipeline/kinesis-ingest:
      metadata:
        component: kinesis
        type: real
      vars:
        stream_name: "${tenant}-${environment}-data-ingest"
        description: "Main data ingestion stream"

        # Stream configuration
        stream_mode: "${kinesis_stream_mode | default('ON_DEMAND')}"
        # For provisioned mode:
        shard_count: "${kinesis_shard_count | default(2)}"

        # Retention
        retention_period: "${kinesis_retention_hours | default(24)}"

        # Encryption
        encryption_type: "KMS"
        kms_key_id: "${kinesis_kms_key_id}"

        # Enhanced monitoring
        shard_level_metrics:
          - "IncomingBytes"
          - "IncomingRecords"
          - "OutgoingBytes"
          - "OutgoingRecords"
          - "WriteProvisionedThroughputExceeded"
          - "ReadProvisionedThroughputExceeded"
          - "IteratorAgeMilliseconds"

        # Consumer configuration
        enhanced_consumers:
          - name: "lambda-processor"
            description: "Lambda consumer for real-time processing"
          - name: "firehose-delivery"
            description: "Firehose consumer for S3 delivery"

      tags:
        DataPipeline: "ingestion"
        StreamType: "ingest"

    data-pipeline/kinesis-enriched:
      metadata:
        component: kinesis
        type: real
      vars:
        stream_name: "${tenant}-${environment}-data-enriched"
        description: "Enriched data stream after transformation"

        # Stream configuration
        stream_mode: "${kinesis_stream_mode | default('ON_DEMAND')}"
        shard_count: "${kinesis_enriched_shard_count | default(2)}"

        # Retention
        retention_period: "${kinesis_retention_hours | default(24)}"

        # Encryption
        encryption_type: "KMS"
        kms_key_id: "${kinesis_kms_key_id}"

        # Enhanced monitoring
        shard_level_metrics:
          - "IncomingBytes"
          - "IncomingRecords"
          - "OutgoingBytes"
          - "OutgoingRecords"
          - "IteratorAgeMilliseconds"

      tags:
        DataPipeline: "transformation"
        StreamType: "enriched"

    # =========================================================================
    # DATA TRANSFORMATION (Lambda)
    # =========================================================================

    data-pipeline/lambda-transformer:
      metadata:
        component: lambda
        type: real
      depends_on:
        - data-pipeline/kinesis-ingest
        - data-pipeline/kinesis-enriched
      vars:
        function_name: "data-transformer"
        description: "Transforms and enriches incoming data"

        # Runtime configuration
        runtime: "${lambda_runtime | default('python3.11')}"
        handler: "handler.process_records"
        timeout: 300
        memory_size: "${transformer_memory | default(1024)}"

        # Reserved concurrency
        reserved_concurrent_executions: "${transformer_concurrency | default(100)}"

        # VPC configuration (if needed for enrichment lookups)
        vpc_id: "${vpc_id}"
        subnet_ids: "${private_subnet_ids}"

        # Environment variables
        environment_variables:
          ENVIRONMENT: "${environment}"
          OUTPUT_STREAM: "${output.data-pipeline/kinesis-enriched.stream_name}"
          ENRICHMENT_TABLE: "${output.data-pipeline/dynamodb-lookup.table_name}"
          LOG_LEVEL: "${log_level | default('INFO')}"

        # Kinesis event source mapping
        event_source_mappings:
          kinesis:
            event_source_arn: "${output.data-pipeline/kinesis-ingest.stream_arn}"
            starting_position: "LATEST"
            batch_size: "${transformer_batch_size | default(100)}"
            maximum_batching_window_in_seconds: 5
            parallelization_factor: 10
            maximum_retry_attempts: 3
            maximum_record_age_in_seconds: 3600
            bisect_batch_on_function_error: true
            tumbling_window_in_seconds: 0
            destination_config:
              on_failure:
                destination_arn: "${dlq_arn}"

        # X-Ray tracing
        tracing_mode: "Active"

        # CloudWatch logs
        log_retention_days: "${log_retention_days | default(30)}"
        kms_key_id: "${lambda_logs_kms_key_id}"

        # Performance alarms
        create_performance_alarms: true
        duration_alarm_threshold: 250000  # 250 seconds
        error_rate_alarm_threshold: 5
        throttle_alarm_threshold: 10
        sns_topic_arn: "${alarm_sns_topic_arn}"

      tags:
        DataPipeline: "transformation"
        Function: "transformer"

    data-pipeline/lambda-validator:
      metadata:
        component: lambda
        type: real
      depends_on:
        - data-pipeline/kinesis-enriched
      vars:
        function_name: "data-validator"
        description: "Validates data schema and quality"

        # Runtime configuration
        runtime: "${lambda_runtime | default('python3.11')}"
        handler: "handler.validate_records"
        timeout: 60
        memory_size: "${validator_memory | default(512)}"

        # Reserved concurrency
        reserved_concurrent_executions: "${validator_concurrency | default(50)}"

        # Environment variables
        environment_variables:
          ENVIRONMENT: "${environment}"
          SCHEMA_REGISTRY_URL: "${schema_registry_url}"
          VALID_RECORDS_STREAM: "${valid_records_stream_name}"
          INVALID_RECORDS_BUCKET: "${output.data-pipeline/s3-raw.bucket_name}"
          INVALID_RECORDS_PREFIX: "invalid/"
          LOG_LEVEL: "${log_level | default('INFO')}"

        # Kinesis event source mapping
        event_source_mappings:
          kinesis:
            event_source_arn: "${output.data-pipeline/kinesis-enriched.stream_arn}"
            starting_position: "LATEST"
            batch_size: "${validator_batch_size | default(50)}"
            maximum_batching_window_in_seconds: 3
            parallelization_factor: 5
            maximum_retry_attempts: 2
            bisect_batch_on_function_error: true

        # X-Ray tracing
        tracing_mode: "Active"

        # CloudWatch logs
        log_retention_days: "${log_retention_days | default(30)}"

      tags:
        DataPipeline: "validation"
        Function: "validator"

    # =========================================================================
    # DATA DELIVERY (Kinesis Firehose)
    # =========================================================================

    data-pipeline/firehose-raw:
      metadata:
        component: firehose
        type: real
      depends_on:
        - data-pipeline/kinesis-ingest
        - data-pipeline/s3-raw
      vars:
        delivery_stream_name: "${tenant}-${environment}-firehose-raw"
        description: "Delivers raw data to S3 data lake"

        # Source configuration
        kinesis_source_configuration:
          kinesis_stream_arn: "${output.data-pipeline/kinesis-ingest.stream_arn}"
          role_arn: "${firehose_kinesis_role_arn}"

        # S3 destination
        s3_configuration:
          bucket_arn: "${output.data-pipeline/s3-raw.bucket_arn}"
          prefix: "data/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/hour=!{timestamp:HH}/"
          error_output_prefix: "errors/!{firehose:error-output-type}/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/"
          buffering_hints:
            size_in_mb: "${firehose_buffer_size | default(128)}"
            interval_in_seconds: "${firehose_buffer_interval | default(300)}"
          compression_format: "GZIP"
          kms_key_arn: "${data_lake_kms_key_id}"

        # Data format conversion (Parquet)
        data_format_conversion:
          enabled: true
          input_format_configuration:
            deserializer:
              hive_json_ser_de: {}
          output_format_configuration:
            serializer:
              parquet_ser_de:
                compression: "SNAPPY"
                enable_dictionary_compression: true
                max_padding_bytes: 0
                page_size_bytes: 1048576
                writer_version: "V2"
          schema_configuration:
            database_name: "${output.data-pipeline/glue-database.database_name}"
            table_name: "raw_events"
            region: "${region}"
            role_arn: "${firehose_glue_role_arn}"

        # CloudWatch logging
        cloudwatch_logging_options:
          enabled: true
          log_group_name: "/aws/kinesisfirehose/${tenant}-${environment}-firehose-raw"
          log_stream_name: "delivery"

        # Processing configuration (optional Lambda transformation)
        processing_configuration:
          enabled: "${enable_firehose_transformation | default(false)}"
          processors:
            - type: "Lambda"
              parameters:
                - name: "LambdaArn"
                  value: "${firehose_transformer_lambda_arn}"
                - name: "BufferSizeInMBs"
                  value: "1"
                - name: "BufferIntervalInSeconds"
                  value: "60"

      tags:
        DataPipeline: "delivery"
        Destination: "s3-raw"

    data-pipeline/firehose-processed:
      metadata:
        component: firehose
        type: real
      depends_on:
        - data-pipeline/kinesis-enriched
        - data-pipeline/s3-processed
      vars:
        delivery_stream_name: "${tenant}-${environment}-firehose-processed"
        description: "Delivers processed data to S3 data lake"

        # Source configuration
        kinesis_source_configuration:
          kinesis_stream_arn: "${output.data-pipeline/kinesis-enriched.stream_arn}"
          role_arn: "${firehose_kinesis_role_arn}"

        # S3 destination
        s3_configuration:
          bucket_arn: "${output.data-pipeline/s3-processed.bucket_arn}"
          prefix: "data/source=!{partitionKeyFromQuery:source}/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/"
          error_output_prefix: "errors/!{firehose:error-output-type}/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/"
          buffering_hints:
            size_in_mb: "${firehose_buffer_size | default(128)}"
            interval_in_seconds: "${firehose_buffer_interval | default(300)}"
          compression_format: "UNCOMPRESSED"  # Parquet handles compression
          kms_key_arn: "${data_lake_kms_key_id}"

        # Data format conversion (Parquet)
        data_format_conversion:
          enabled: true
          input_format_configuration:
            deserializer:
              open_x_json_ser_de:
                convert_dots_in_json_keys_to_underscores: true
                case_insensitive: true
          output_format_configuration:
            serializer:
              parquet_ser_de:
                compression: "SNAPPY"
                enable_dictionary_compression: true
          schema_configuration:
            database_name: "${output.data-pipeline/glue-database.database_name}"
            table_name: "processed_events"
            region: "${region}"
            role_arn: "${firehose_glue_role_arn}"

        # Dynamic partitioning
        dynamic_partitioning_configuration:
          enabled: true
          retry_options:
            duration_in_seconds: 300

        # CloudWatch logging
        cloudwatch_logging_options:
          enabled: true
          log_group_name: "/aws/kinesisfirehose/${tenant}-${environment}-firehose-processed"
          log_stream_name: "delivery"

      tags:
        DataPipeline: "delivery"
        Destination: "s3-processed"

    # =========================================================================
    # DATA CATALOG (Glue)
    # =========================================================================

    data-pipeline/glue-database:
      metadata:
        component: glue
        type: real
      vars:
        database_name: "${tenant}_${environment}_data_lake"
        description: "Data lake catalog database"

        # Location
        location_uri: "s3://${output.data-pipeline/s3-curated.bucket_name}/"

        # Permissions
        create_table_default_permissions:
          - principal:
              data_lake_principal_identifier: "IAM_ALLOWED_PRINCIPALS"
            permissions:
              - "ALL"

      tags:
        DataPipeline: "catalog"
        CatalogType: "database"

    data-pipeline/glue-crawlers:
      metadata:
        component: glue-crawler
        type: real
      depends_on:
        - data-pipeline/glue-database
        - data-pipeline/s3-raw
        - data-pipeline/s3-processed
        - data-pipeline/s3-curated
      vars:
        crawlers:
          # Raw data crawler
          raw_data:
            name: "${tenant}-${environment}-raw-data-crawler"
            description: "Crawls raw data in S3"
            database_name: "${output.data-pipeline/glue-database.database_name}"
            role: "${glue_crawler_role_arn}"
            schedule: "cron(0 */6 * * ? *)"  # Every 6 hours
            s3_targets:
              - path: "s3://${output.data-pipeline/s3-raw.bucket_name}/data/"
                exclusions:
                  - "errors/**"
            schema_change_policy:
              delete_behavior: "LOG"
              update_behavior: "UPDATE_IN_DATABASE"
            configuration:
              Version: 1.0
              CrawlerOutput:
                Partitions:
                  AddOrUpdateBehavior: "InheritFromTable"
              Grouping:
                TableGroupingPolicy: "CombineCompatibleSchemas"
            table_prefix: "raw_"

          # Processed data crawler
          processed_data:
            name: "${tenant}-${environment}-processed-data-crawler"
            description: "Crawls processed data in S3"
            database_name: "${output.data-pipeline/glue-database.database_name}"
            role: "${glue_crawler_role_arn}"
            schedule: "cron(0 */6 * * ? *)"  # Every 6 hours
            s3_targets:
              - path: "s3://${output.data-pipeline/s3-processed.bucket_name}/data/"
                exclusions:
                  - "errors/**"
            schema_change_policy:
              delete_behavior: "LOG"
              update_behavior: "UPDATE_IN_DATABASE"
            table_prefix: "processed_"

          # Curated data crawler
          curated_data:
            name: "${tenant}-${environment}-curated-data-crawler"
            description: "Crawls curated data in S3"
            database_name: "${output.data-pipeline/glue-database.database_name}"
            role: "${glue_crawler_role_arn}"
            schedule: "cron(0 0 * * ? *)"  # Daily
            s3_targets:
              - path: "s3://${output.data-pipeline/s3-curated.bucket_name}/"
            schema_change_policy:
              delete_behavior: "LOG"
              update_behavior: "UPDATE_IN_DATABASE"
            table_prefix: "curated_"

      tags:
        DataPipeline: "catalog"
        CatalogType: "crawler"

    # =========================================================================
    # QUERY ENGINE (Athena)
    # =========================================================================

    data-pipeline/athena:
      metadata:
        component: athena
        type: real
      depends_on:
        - data-pipeline/glue-database
      vars:
        workgroup_name: "${tenant}-${environment}-data-pipeline"
        description: "Athena workgroup for data pipeline queries"

        # Output configuration
        output_location: "s3://${tenant}-${environment}-athena-results/"

        # Configuration
        enforce_workgroup_configuration: true
        publish_cloudwatch_metrics_enabled: true
        bytes_scanned_cutoff_per_query: "${athena_bytes_limit | default(10737418240)}"  # 10 GB

        # Encryption
        encryption_configuration:
          encryption_option: "SSE_KMS"
          kms_key_arn: "${athena_kms_key_arn}"

        # Engine version
        engine_version: "Athena engine version 3"

        # Requester pays (for cross-account queries)
        requester_pays_enabled: false

        # Named queries (saved queries)
        named_queries:
          daily_summary:
            name: "Daily Summary"
            description: "Daily event summary by source"
            database: "${output.data-pipeline/glue-database.database_name}"
            query: |
              SELECT
                source,
                DATE(event_time) as event_date,
                COUNT(*) as event_count,
                COUNT(DISTINCT user_id) as unique_users
              FROM processed_events
              WHERE year = CAST(YEAR(CURRENT_DATE) AS VARCHAR)
                AND month = LPAD(CAST(MONTH(CURRENT_DATE) AS VARCHAR), 2, '0')
              GROUP BY source, DATE(event_time)
              ORDER BY event_date DESC, event_count DESC

          error_analysis:
            name: "Error Analysis"
            description: "Analyze error patterns"
            database: "${output.data-pipeline/glue-database.database_name}"
            query: |
              SELECT
                error_code,
                error_message,
                COUNT(*) as error_count,
                MIN(event_time) as first_occurrence,
                MAX(event_time) as last_occurrence
              FROM raw_events
              WHERE error_code IS NOT NULL
                AND year = CAST(YEAR(CURRENT_DATE) AS VARCHAR)
              GROUP BY error_code, error_message
              ORDER BY error_count DESC
              LIMIT 100

          data_quality_check:
            name: "Data Quality Check"
            description: "Check for data quality issues"
            database: "${output.data-pipeline/glue-database.database_name}"
            query: |
              SELECT
                'null_user_id' as check_name,
                COUNT(*) as issue_count
              FROM processed_events
              WHERE user_id IS NULL
              UNION ALL
              SELECT
                'future_events' as check_name,
                COUNT(*) as issue_count
              FROM processed_events
              WHERE event_time > CURRENT_TIMESTAMP
              UNION ALL
              SELECT
                'missing_required_fields' as check_name,
                COUNT(*) as issue_count
              FROM processed_events
              WHERE event_type IS NULL OR source IS NULL

      tags:
        DataPipeline: "analytics"
        QueryEngine: "athena"

    # =========================================================================
    # WORKFLOW ORCHESTRATION (Step Functions)
    # =========================================================================

    data-pipeline/step-functions:
      metadata:
        component: step-functions
        type: real
      depends_on:
        - data-pipeline/glue-crawlers
        - data-pipeline/athena
      vars:
        state_machines:
          # Daily ETL workflow
          daily_etl:
            name: "${tenant}-${environment}-daily-etl"
            description: "Daily ETL workflow for data processing"
            type: "STANDARD"
            logging_configuration:
              level: "ALL"
              include_execution_data: true
              log_destination: "arn:aws:logs:${region}:${account_id}:log-group:/aws/states/${tenant}-${environment}-daily-etl:*"
            tracing_configuration:
              enabled: true
            definition:
              Comment: "Daily ETL workflow"
              StartAt: "RunRawCrawler"
              States:
                RunRawCrawler:
                  Type: "Task"
                  Resource: "arn:aws:states:::glue:startJobRun.sync"
                  Parameters:
                    CrawlerName: "${tenant}-${environment}-raw-data-crawler"
                  Next: "CheckRawCrawlerStatus"
                  Catch:
                    - ErrorEquals:
                        - "States.ALL"
                      Next: "NotifyFailure"

                CheckRawCrawlerStatus:
                  Type: "Choice"
                  Choices:
                    - Variable: "$.CrawlerStatus"
                      StringEquals: "SUCCEEDED"
                      Next: "RunTransformationJob"
                  Default: "NotifyFailure"

                RunTransformationJob:
                  Type: "Task"
                  Resource: "arn:aws:states:::glue:startJobRun.sync"
                  Parameters:
                    JobName: "${transformation_glue_job_name}"
                    Arguments:
                      "--input_path": "s3://${output.data-pipeline/s3-raw.bucket_name}/data/"
                      "--output_path": "s3://${output.data-pipeline/s3-processed.bucket_name}/data/"
                      "--date.$": "$.execution_date"
                  Next: "RunProcessedCrawler"
                  Catch:
                    - ErrorEquals:
                        - "States.ALL"
                      Next: "NotifyFailure"

                RunProcessedCrawler:
                  Type: "Task"
                  Resource: "arn:aws:states:::glue:startJobRun.sync"
                  Parameters:
                    CrawlerName: "${tenant}-${environment}-processed-data-crawler"
                  Next: "RunDataQualityChecks"
                  Catch:
                    - ErrorEquals:
                        - "States.ALL"
                      Next: "NotifyFailure"

                RunDataQualityChecks:
                  Type: "Task"
                  Resource: "arn:aws:states:::athena:startQueryExecution.sync"
                  Parameters:
                    QueryString: "SELECT COUNT(*) FROM ${output.data-pipeline/glue-database.database_name}.processed_events WHERE year = CAST(YEAR(CURRENT_DATE) AS VARCHAR)"
                    WorkGroup: "${output.data-pipeline/athena.workgroup_name}"
                  Next: "CreateCuratedDataset"
                  Catch:
                    - ErrorEquals:
                        - "States.ALL"
                      Next: "NotifyFailure"

                CreateCuratedDataset:
                  Type: "Task"
                  Resource: "arn:aws:states:::glue:startJobRun.sync"
                  Parameters:
                    JobName: "${curation_glue_job_name}"
                    Arguments:
                      "--input_path": "s3://${output.data-pipeline/s3-processed.bucket_name}/data/"
                      "--output_path": "s3://${output.data-pipeline/s3-curated.bucket_name}/"
                      "--date.$": "$.execution_date"
                  Next: "RunCuratedCrawler"
                  Catch:
                    - ErrorEquals:
                        - "States.ALL"
                      Next: "NotifyFailure"

                RunCuratedCrawler:
                  Type: "Task"
                  Resource: "arn:aws:states:::glue:startJobRun.sync"
                  Parameters:
                    CrawlerName: "${tenant}-${environment}-curated-data-crawler"
                  Next: "NotifySuccess"
                  Catch:
                    - ErrorEquals:
                        - "States.ALL"
                      Next: "NotifyFailure"

                NotifySuccess:
                  Type: "Task"
                  Resource: "arn:aws:states:::sns:publish"
                  Parameters:
                    TopicArn: "${data_pipeline_sns_topic_arn}"
                    Message:
                      Status: "SUCCESS"
                      Pipeline: "Daily ETL"
                      ExecutionId.$: "$$.Execution.Id"
                      CompletedAt.$: "$$.State.EnteredTime"
                  End: true

                NotifyFailure:
                  Type: "Task"
                  Resource: "arn:aws:states:::sns:publish"
                  Parameters:
                    TopicArn: "${data_pipeline_sns_topic_arn}"
                    Message:
                      Status: "FAILED"
                      Pipeline: "Daily ETL"
                      ExecutionId.$: "$$.Execution.Id"
                      Error.$: "$.Error"
                      Cause.$: "$.Cause"
                  Next: "FailState"

                FailState:
                  Type: "Fail"
                  Error: "ETLFailed"
                  Cause: "Daily ETL workflow failed"

          # Data quality workflow
          data_quality:
            name: "${tenant}-${environment}-data-quality"
            description: "Data quality validation workflow"
            type: "EXPRESS"
            logging_configuration:
              level: "ERROR"
              include_execution_data: false
            definition:
              Comment: "Data quality validation"
              StartAt: "ValidateSchema"
              States:
                ValidateSchema:
                  Type: "Task"
                  Resource: "${schema_validation_lambda_arn}"
                  Next: "ValidateCompleteness"
                ValidateCompleteness:
                  Type: "Task"
                  Resource: "${completeness_validation_lambda_arn}"
                  Next: "ValidateAccuracy"
                ValidateAccuracy:
                  Type: "Task"
                  Resource: "${accuracy_validation_lambda_arn}"
                  End: true

      tags:
        DataPipeline: "orchestration"
        Workflow: "step-functions"

    # =========================================================================
    # SCHEDULING (EventBridge)
    # =========================================================================

    data-pipeline/eventbridge:
      metadata:
        component: eventbridge
        type: real
      depends_on:
        - data-pipeline/step-functions
      vars:
        rules:
          # Daily ETL trigger
          daily_etl_trigger:
            name: "${tenant}-${environment}-daily-etl-trigger"
            description: "Triggers daily ETL workflow"
            schedule_expression: "cron(0 2 * * ? *)"  # 2 AM UTC daily
            state: "ENABLED"
            targets:
              - id: "daily-etl-state-machine"
                arn: "${output.data-pipeline/step-functions.state_machine_arns.daily_etl}"
                role_arn: "${eventbridge_step_functions_role_arn}"
                input:
                  execution_date.$: "$.time"

          # Hourly data quality check
          hourly_quality_check:
            name: "${tenant}-${environment}-hourly-quality-check"
            description: "Triggers hourly data quality checks"
            schedule_expression: "rate(1 hour)"
            state: "${enable_hourly_quality_check | default('DISABLED')}"
            targets:
              - id: "data-quality-state-machine"
                arn: "${output.data-pipeline/step-functions.state_machine_arns.data_quality}"
                role_arn: "${eventbridge_step_functions_role_arn}"

          # Crawler completion events
          crawler_completion:
            name: "${tenant}-${environment}-crawler-completion"
            description: "Handle Glue crawler completion events"
            event_pattern:
              source:
                - "aws.glue"
              detail-type:
                - "Glue Crawler State Change"
              detail:
                state:
                  - "Succeeded"
                  - "Failed"
                crawlerName:
                  - prefix: "${tenant}-${environment}"
            targets:
              - id: "process-crawler-completion"
                arn: "${crawler_completion_lambda_arn}"

      tags:
        DataPipeline: "scheduling"
        Orchestration: "eventbridge"

    # =========================================================================
    # LOOKUP DATA (DynamoDB)
    # =========================================================================

    data-pipeline/dynamodb-lookup:
      metadata:
        component: dynamodb
        type: real
      vars:
        tables:
          lookup:
            name: "${tenant}-${environment}-data-lookup"
            billing_mode: "PAY_PER_REQUEST"
            hash_key: "lookup_type"
            range_key: "lookup_key"
            attributes:
              - name: "lookup_type"
                type: "S"
              - name: "lookup_key"
                type: "S"
            point_in_time_recovery: true
            server_side_encryption:
              enabled: true
              kms_key_arn: "${dynamodb_kms_key_arn}"
            ttl:
              enabled: true
              attribute_name: "expires_at"

      tags:
        DataPipeline: "enrichment"
        DataStore: "lookup"

    # =========================================================================
    # MONITORING
    # =========================================================================

    data-pipeline/monitoring:
      metadata:
        component: monitoring
        type: real
        inherits:
          - monitoring/defaults
      depends_on:
        - data-pipeline/kinesis-ingest
        - data-pipeline/lambda-transformer
        - data-pipeline/firehose-raw
        - data-pipeline/step-functions
      vars:
        dashboard_name: "${tenant}-${environment}-data-pipeline-dashboard"

        # Enable all monitoring features
        enable_resource_monitoring: true
        enable_cost_monitoring: true

        # Custom dashboard
        dashboards:
          data_pipeline_overview:
            name: "${tenant}-${environment}-data-pipeline-overview"
            widgets:
              # Kinesis metrics
              - type: "metric"
                properties:
                  title: "Kinesis Ingestion Rate"
                  metrics:
                    - namespace: "AWS/Kinesis"
                      metric: "IncomingRecords"
                      dimensions:
                        StreamName: "${output.data-pipeline/kinesis-ingest.stream_name}"
                      stat: "Sum"
                      period: 60
                    - namespace: "AWS/Kinesis"
                      metric: "IncomingBytes"
                      dimensions:
                        StreamName: "${output.data-pipeline/kinesis-ingest.stream_name}"
                      stat: "Sum"
                      period: 60

              # Lambda metrics
              - type: "metric"
                properties:
                  title: "Lambda Transformer Performance"
                  metrics:
                    - namespace: "AWS/Lambda"
                      metric: "Duration"
                      dimensions:
                        FunctionName: "${output.data-pipeline/lambda-transformer.function_name}"
                      stat: "Average"
                      period: 60
                    - namespace: "AWS/Lambda"
                      metric: "Errors"
                      dimensions:
                        FunctionName: "${output.data-pipeline/lambda-transformer.function_name}"
                      stat: "Sum"
                      period: 60

              # Firehose metrics
              - type: "metric"
                properties:
                  title: "Firehose Delivery"
                  metrics:
                    - namespace: "AWS/Firehose"
                      metric: "DeliveryToS3.Records"
                      dimensions:
                        DeliveryStreamName: "${output.data-pipeline/firehose-raw.delivery_stream_name}"
                      stat: "Sum"
                      period: 300
                    - namespace: "AWS/Firehose"
                      metric: "DeliveryToS3.Success"
                      dimensions:
                        DeliveryStreamName: "${output.data-pipeline/firehose-raw.delivery_stream_name}"
                      stat: "Average"
                      period: 300

              # Step Functions metrics
              - type: "metric"
                properties:
                  title: "ETL Workflow Status"
                  metrics:
                    - namespace: "AWS/States"
                      metric: "ExecutionsSucceeded"
                      dimensions:
                        StateMachineArn: "${output.data-pipeline/step-functions.state_machine_arns.daily_etl}"
                      stat: "Sum"
                      period: 86400
                    - namespace: "AWS/States"
                      metric: "ExecutionsFailed"
                      dimensions:
                        StateMachineArn: "${output.data-pipeline/step-functions.state_machine_arns.daily_etl}"
                      stat: "Sum"
                      period: 86400

        # Alarms
        alarms:
          # Kinesis alarms
          kinesis_iterator_age:
            name: "${tenant}-${environment}-kinesis-iterator-age"
            metric_name: "GetRecords.IteratorAgeMilliseconds"
            namespace: "AWS/Kinesis"
            comparison_operator: "GreaterThanThreshold"
            threshold: 60000  # 1 minute
            evaluation_periods: 3
            period: 300
            statistic: "Maximum"
            dimensions:
              StreamName: "${output.data-pipeline/kinesis-ingest.stream_name}"

          kinesis_write_throttled:
            name: "${tenant}-${environment}-kinesis-write-throttled"
            metric_name: "WriteProvisionedThroughputExceeded"
            namespace: "AWS/Kinesis"
            comparison_operator: "GreaterThanThreshold"
            threshold: 0
            evaluation_periods: 1
            period: 60
            statistic: "Sum"
            dimensions:
              StreamName: "${output.data-pipeline/kinesis-ingest.stream_name}"

          # Lambda alarms
          lambda_errors:
            name: "${tenant}-${environment}-transformer-errors"
            metric_name: "Errors"
            namespace: "AWS/Lambda"
            comparison_operator: "GreaterThanThreshold"
            threshold: 5
            evaluation_periods: 2
            period: 300
            statistic: "Sum"
            dimensions:
              FunctionName: "${output.data-pipeline/lambda-transformer.function_name}"

          # Firehose alarms
          firehose_delivery_failed:
            name: "${tenant}-${environment}-firehose-delivery-failed"
            metric_name: "DeliveryToS3.DataFreshness"
            namespace: "AWS/Firehose"
            comparison_operator: "GreaterThanThreshold"
            threshold: 900  # 15 minutes
            evaluation_periods: 2
            period: 300
            statistic: "Maximum"
            dimensions:
              DeliveryStreamName: "${output.data-pipeline/firehose-raw.delivery_stream_name}"

          # Step Functions alarms
          etl_failed:
            name: "${tenant}-${environment}-etl-failed"
            metric_name: "ExecutionsFailed"
            namespace: "AWS/States"
            comparison_operator: "GreaterThanThreshold"
            threshold: 0
            evaluation_periods: 1
            period: 86400
            statistic: "Sum"
            dimensions:
              StateMachineArn: "${output.data-pipeline/step-functions.state_machine_arns.daily_etl}"

        # SNS notifications
        alarm_notifications_enabled: true
        alarm_email_addresses: "${alarm_email_addresses | default([])}"
        sns_topic_name: "${tenant}-${environment}-data-pipeline-alarms"

      tags:
        DataPipeline: "monitoring"

# =============================================================================
# ENVIRONMENT-SPECIFIC OVERRIDES
# =============================================================================
#
# Development:
#   vars:
#     kinesis_stream_mode: "ON_DEMAND"
#     kinesis_retention_hours: 24
#     firehose_buffer_size: 64
#     firehose_buffer_interval: 60
#     transformer_memory: 512
#     transformer_concurrency: 10
#     athena_bytes_limit: 1073741824  # 1 GB
#     enable_cross_region_replication: false
#
# Staging:
#   vars:
#     kinesis_stream_mode: "ON_DEMAND"
#     kinesis_retention_hours: 48
#     firehose_buffer_size: 128
#     firehose_buffer_interval: 300
#     transformer_memory: 1024
#     transformer_concurrency: 50
#     athena_bytes_limit: 5368709120  # 5 GB
#     enable_cross_region_replication: false
#
# Production:
#   vars:
#     kinesis_stream_mode: "PROVISIONED"
#     kinesis_shard_count: 10
#     kinesis_retention_hours: 168  # 7 days
#     firehose_buffer_size: 128
#     firehose_buffer_interval: 300
#     transformer_memory: 2048
#     transformer_concurrency: 200
#     athena_bytes_limit: 107374182400  # 100 GB
#     enable_cross_region_replication: true
#     enable_hourly_quality_check: "ENABLED"
#     log_retention_days: 90

# =============================================================================
# REQUIRED VARIABLES
# =============================================================================
# data_lake_kms_key_id: "arn:aws:kms:..."
# kinesis_kms_key_id: "arn:aws:kms:..."
# lambda_logs_kms_key_id: "arn:aws:kms:..."
# athena_kms_key_arn: "arn:aws:kms:..."
# dynamodb_kms_key_arn: "arn:aws:kms:..."
# log_bucket_name: "my-log-bucket"
# glue_crawler_role_arn: "arn:aws:iam::..."
# firehose_kinesis_role_arn: "arn:aws:iam::..."
# firehose_glue_role_arn: "arn:aws:iam::..."
# eventbridge_step_functions_role_arn: "arn:aws:iam::..."
# transformation_glue_job_name: "my-transformation-job"
# curation_glue_job_name: "my-curation-job"
# data_pipeline_sns_topic_arn: "arn:aws:sns:..."
# dlq_arn: "arn:aws:sqs:..."
# alarm_email_addresses: ["ops@example.com"]
