---
# Streaming Pipeline Pattern
# Real-time data streaming and processing infrastructure
# Version: 1.0.0

name: streaming-pipeline-pattern
description: "Real-time data streaming pipeline with Kinesis, Lambda processing, Firehose delivery, and comprehensive monitoring"
version: "1.0.0"
maturity: "production"

# =============================================================================
# PATTERN METADATA
# =============================================================================
pattern:
  category: "patterns/data-pipelines"
  tier: "enterprise"

  use_cases:
    - "Real-time analytics"
    - "Event streaming"
    - "IoT data processing"
    - "Log aggregation and analysis"
    - "Clickstream analysis"
    - "Fraud detection"
    - "Real-time recommendations"
    - "Live dashboards"
    - "Change data capture (CDC)"
    - "Metrics collection"

  architecture:
    overview: |
      This pattern implements a complete real-time data streaming pipeline using
      AWS Kinesis services. Data flows from producers through Kinesis Data Streams,
      gets processed by Lambda functions or Kinesis Data Analytics, and is delivered
      to multiple destinations via Kinesis Firehose. The architecture supports
      high throughput, exactly-once processing semantics, and comprehensive
      observability.

    components:
      - name: "Ingestion Layer"
        components: ["kinesis-data-streams", "api-gateway", "kinesis-agent"]
        description: "High-throughput data ingestion"
      - name: "Processing Layer"
        components: ["lambda-processors", "kinesis-analytics"]
        description: "Real-time stream processing"
      - name: "Delivery Layer"
        components: ["kinesis-firehose", "lambda-transformers"]
        description: "Data delivery to destinations"
      - name: "Storage Layer"
        components: ["s3-data-lake", "opensearch", "redshift", "dynamodb"]
        description: "Persistent data storage"
      - name: "State Management"
        components: ["dynamodb-state", "elasticache"]
        description: "Processing state and caching"
      - name: "Observability"
        components: ["cloudwatch-dashboards", "alarms", "xray"]
        description: "Monitoring and alerting"

    data_flow:
      - step: 1
        name: "Data Ingestion"
        description: "Producers send data to Kinesis Data Streams"
      - step: 2
        name: "Stream Processing"
        description: "Lambda/Kinesis Analytics processes records"
      - step: 3
        name: "Data Enrichment"
        description: "Records enriched with reference data"
      - step: 4
        name: "Data Transformation"
        description: "Firehose transforms data format"
      - step: 5
        name: "Data Delivery"
        description: "Data delivered to S3/OpenSearch/Redshift"
      - step: 6
        name: "Error Handling"
        description: "Failed records sent to DLQ for retry"

  estimated_cost:
    minimum: "$50/month"
    typical: "$300/month"
    maximum: "$5000+/month"
    breakdown:
      kinesis_data_streams: "$0.036/shard-hour + $0.014/million PUT payload units"
      kinesis_firehose: "$0.029/GB ingested"
      kinesis_analytics: "$0.11/KPU-hour"
      lambda: "$0.20/million requests + compute"
      s3: "$0.023/GB storage"
      opensearch: "Instance-based pricing"
      cloudwatch: "$3/dashboard + logs + metrics"
    optimization_tips:
      - "Use ON_DEMAND capacity mode for variable workloads"
      - "Implement data compression before ingestion"
      - "Use enhanced fan-out only when needed"
      - "Batch records to reduce PUT operations"
      - "Use S3 Intelligent-Tiering for long-term storage"
      - "Right-size Kinesis Analytics KPUs"

  deployment_time: "25-40 minutes"

  complexity: "advanced"

  prerequisites:
    - "VPC with private subnets"
    - "IAM permissions for Kinesis, Lambda, Firehose, S3"
    - "KMS key for encryption"
    - "OpenSearch domain (optional)"
    - "Redshift cluster (optional)"

# =============================================================================
# STACK CONFIGURATION
# =============================================================================
import:
  - catalog/_base/defaults
  - catalog/iam/defaults
  - catalog/monitoring/defaults

vars:
  # Common pattern variables
  pattern_name: "streaming-pipeline"
  enable_enhanced_fanout: true
  enable_analytics: true
  enable_opensearch: true
  enable_redshift: false
  data_retention_hours: 168  # 7 days
  archive_retention_days: 365
  log_retention_days: 30

components:
  terraform:
    # =========================================================================
    # INGESTION LAYER
    # =========================================================================
    kinesis-ingest:
      component: "kinesis"
      vars:
        enabled: true

        stream_name: "${tenant}-${environment}-ingest-stream"
        description: "Primary data ingestion stream"

        # Capacity Mode
        stream_mode: "ON_DEMAND"
        # For PROVISIONED mode:
        # stream_mode: "PROVISIONED"
        # shard_count: 4

        # Data Retention
        retention_period: "${data_retention_hours}"

        # Enhanced Fan-Out
        enable_enhanced_fanout: "${enable_enhanced_fanout}"
        enhanced_fanout_consumers:
          - name: "${tenant}-${environment}-processor-consumer"
            description: "Consumer for Lambda processor"
          - name: "${tenant}-${environment}-analytics-consumer"
            description: "Consumer for Kinesis Analytics"

        # Encryption
        encryption_type: "KMS"
        kms_key_id: "${output.kms.key_arn}"

        # Stream Level Metrics
        shard_level_metrics:
          - "IncomingBytes"
          - "IncomingRecords"
          - "OutgoingBytes"
          - "OutgoingRecords"
          - "WriteProvisionedThroughputExceeded"
          - "ReadProvisionedThroughputExceeded"
          - "IteratorAgeMilliseconds"

      tags:
        StackType: "streaming-pipeline"
        Layer: "ingestion"

    kinesis-enriched:
      component: "kinesis"
      vars:
        enabled: true

        stream_name: "${tenant}-${environment}-enriched-stream"
        description: "Stream for enriched/processed data"
        stream_mode: "ON_DEMAND"
        retention_period: 24
        encryption_type: "KMS"
        kms_key_id: "${output.kms.key_arn}"

        enable_enhanced_fanout: true
        enhanced_fanout_consumers:
          - name: "${tenant}-${environment}-firehose-consumer"
            description: "Consumer for Firehose delivery"

      tags:
        StackType: "streaming-pipeline"
        Layer: "processing"

    # HTTP/REST ingestion endpoint
    api-ingest:
      component: "apigateway-http"
      vars:
        enabled: true

        api_name: "${tenant}-${environment}-stream-ingest"
        description: "HTTP API for stream data ingestion"
        protocol_type: "HTTP"

        cors_configuration:
          allow_origins: ["*"]
          allow_methods: ["POST", "OPTIONS"]
          allow_headers: ["Content-Type", "X-Partition-Key"]
          max_age: 300

        routes:
          # Single record ingestion
          put-record:
            route_key: "POST /stream"
            integration_type: "AWS_PROXY"
            integration_subtype: "Kinesis-PutRecord"
            credentials_arn: "${output.iam.api_kinesis_role_arn}"
            request_parameters:
              StreamName: "${output.kinesis-ingest.stream_name}"
              Data: "$request.body"
              PartitionKey: "$request.header.X-Partition-Key"

          # Batch record ingestion
          put-records:
            route_key: "POST /stream/batch"
            integration_type: "AWS_PROXY"
            integration_subtype: "Kinesis-PutRecords"
            credentials_arn: "${output.iam.api_kinesis_role_arn}"
            request_parameters:
              StreamName: "${output.kinesis-ingest.stream_name}"
              Records: "$request.body.records"

        stages:
          default:
            name: "$default"
            auto_deploy: true
            access_log_settings:
              destination_arn: "${output.cloudwatch-logs.api_log_group_arn}"
            default_route_settings:
              throttling_burst_limit: 10000
              throttling_rate_limit: 5000

      tags:
        StackType: "streaming-pipeline"
        Layer: "ingestion"

    # =========================================================================
    # PROCESSING LAYER
    # =========================================================================
    lambda-stream-processor:
      component: "lambda"
      depends_on:
        - kinesis-ingest
        - kinesis-enriched
        - dynamodb-lookup
        - iam
      vars:
        enabled: true

        functions:
          # Main stream processor
          stream-processor:
            function_name: "${tenant}-${environment}-stream-processor"
            description: "Process and enrich streaming data"
            runtime: "python3.11"
            handler: "processors/stream.handler"
            memory_size: 1024
            timeout: 60
            architecture: "arm64"

            # Kinesis trigger
            event_source_mapping:
              kinesis:
                event_source_arn: "${output.kinesis-ingest.stream_arn}"
                starting_position: "LATEST"
                batch_size: 100
                maximum_batching_window_in_seconds: 5
                parallelization_factor: 4
                maximum_retry_attempts: 3
                maximum_record_age_in_seconds: 604800  # 7 days
                bisect_batch_on_function_error: true
                function_response_types:
                  - "ReportBatchItemFailures"
                destination_config:
                  on_failure:
                    destination_arn: "${output.sqs-dlq.queue_arn}"
                # For enhanced fan-out:
                # consumer_arn: "${output.kinesis-ingest.processor_consumer_arn}"

            environment_variables:
              ENVIRONMENT: "${environment}"
              OUTPUT_STREAM: "${output.kinesis-enriched.stream_name}"
              LOOKUP_TABLE: "${output.dynamodb-lookup.table_name}"
              CACHE_ENDPOINT: "${output.elasticache.endpoint}"
              LOG_LEVEL: "INFO"
              POWERTOOLS_SERVICE_NAME: "stream-processor"
              POWERTOOLS_METRICS_NAMESPACE: "${tenant}-${environment}-streaming"

            vpc_config:
              subnet_ids: "${output.vpc.private_subnet_ids}"
              security_group_ids:
                - "${output.securitygroup.lambda_sg_id}"

            tracing_config:
              mode: "Active"

            reserved_concurrent_executions: 100

          # Aggregator for time-window aggregations
          stream-aggregator:
            function_name: "${tenant}-${environment}-stream-aggregator"
            description: "Aggregate streaming data over time windows"
            runtime: "python3.11"
            handler: "processors/aggregator.handler"
            memory_size: 512
            timeout: 300
            architecture: "arm64"

            # Triggered by CloudWatch Events (scheduled)
            schedule_expression: "rate(1 minute)"

            environment_variables:
              ENVIRONMENT: "${environment}"
              INPUT_STREAM: "${output.kinesis-enriched.stream_name}"
              AGGREGATION_TABLE: "${output.dynamodb-state.table_name}"
              WINDOW_SIZE_SECONDS: "60"

            tracing_config:
              mode: "Active"

          # DLQ processor
          dlq-processor:
            function_name: "${tenant}-${environment}-stream-dlq-processor"
            description: "Process failed records from DLQ"
            runtime: "python3.11"
            handler: "processors/dlq.handler"
            memory_size: 256
            timeout: 60
            architecture: "arm64"

            event_source_mapping:
              sqs:
                event_source_arn: "${output.sqs-dlq.queue_arn}"
                batch_size: 10
                maximum_batching_window_in_seconds: 30

            environment_variables:
              ENVIRONMENT: "${environment}"
              ALERT_SNS_TOPIC: "${output.sns-alerts.topic_arn}"
              RETRY_STREAM: "${output.kinesis-ingest.stream_name}"

            tracing_config:
              mode: "Active"

        # Lambda Layers
        layers:
          - arn: "arn:aws:lambda:${region}:017000801446:layer:AWSLambdaPowertoolsPythonV2-Arm64:51"

      tags:
        StackType: "streaming-pipeline"
        Layer: "processing"

    # Kinesis Data Analytics (Flink) - Optional
    kinesis-analytics:
      component: "kinesis-analytics"
      depends_on:
        - kinesis-ingest
        - kinesis-enriched
        - s3-code
      vars:
        enabled: "${enable_analytics}"

        application_name: "${tenant}-${environment}-stream-analytics"
        description: "Real-time stream analytics with Apache Flink"
        runtime_environment: "FLINK-1_18"

        application_configuration:
          # Code configuration
          application_code_configuration:
            code_content:
              s3_content_location:
                bucket_arn: "${output.s3-code.bucket_arn}"
                file_key: "flink/stream-analytics.jar"
            code_content_type: "ZIPFILE"

          # Flink configuration
          flink_application_configuration:
            checkpoint_configuration:
              configuration_type: "DEFAULT"
            monitoring_configuration:
              configuration_type: "CUSTOM"
              metrics_level: "APPLICATION"
              log_level: "INFO"
            parallelism_configuration:
              configuration_type: "CUSTOM"
              auto_scaling_enabled: true
              parallelism: 4
              parallelism_per_kpu: 1

          # Environment properties
          environment_properties:
            - property_group_id: "kinesis.streams.input"
              property_map:
                stream.name: "${output.kinesis-ingest.stream_name}"
                aws.region: "${region}"
                flink.stream.initpos: "LATEST"
            - property_group_id: "kinesis.streams.output"
              property_map:
                stream.name: "${output.kinesis-enriched.stream_name}"
                aws.region: "${region}"
            - property_group_id: "application.config"
              property_map:
                window.size.seconds: "60"
                late.arrival.seconds: "30"
                checkpoint.interval.ms: "60000"

          # VPC configuration
          vpc_configuration:
            security_group_ids:
              - "${output.securitygroup.analytics_sg_id}"
            subnet_ids: "${output.vpc.private_subnet_ids}"

        # CloudWatch logging
        cloudwatch_logging_configuration:
          log_group: "${output.cloudwatch-logs.analytics_log_group_name}"
          log_stream: "kinesis-analytics"

        # Service execution role
        service_execution_role: "${output.iam.analytics_role_arn}"

        # Auto-scaling
        auto_scaling_enabled: true

        # Start the application
        start_application: true

      tags:
        StackType: "streaming-pipeline"
        Layer: "processing"

    # =========================================================================
    # DELIVERY LAYER
    # =========================================================================
    firehose-s3:
      component: "firehose"
      depends_on:
        - kinesis-enriched
        - s3-data-lake
        - lambda-firehose-transformer
      vars:
        enabled: true

        delivery_stream_name: "${tenant}-${environment}-s3-delivery"
        delivery_stream_type: "KinesisStreamAsSource"

        # Kinesis source
        kinesis_source_configuration:
          kinesis_stream_arn: "${output.kinesis-enriched.stream_arn}"
          role_arn: "${output.iam.firehose_role_arn}"

        # Extended S3 configuration
        extended_s3_configuration:
          bucket_arn: "${output.s3-data-lake.bucket_arn}"
          role_arn: "${output.iam.firehose_role_arn}"

          # Buffering
          buffering_size: 128  # MB
          buffering_interval: 300  # seconds

          # Compression
          compression_format: "GZIP"

          # Dynamic partitioning
          dynamic_partitioning_configuration:
            enabled: true
          prefix: "data/year=!{partitionKeyFromQuery:year}/month=!{partitionKeyFromQuery:month}/day=!{partitionKeyFromQuery:day}/hour=!{partitionKeyFromQuery:hour}/"
          error_output_prefix: "errors/!{firehose:error-output-type}/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/"

          # Data format conversion
          data_format_conversion_configuration:
            enabled: true
            input_format_configuration:
              deserializer:
                open_x_json_ser_de: {}
            output_format_configuration:
              serializer:
                parquet_ser_de:
                  compression: "GZIP"
            schema_configuration:
              database_name: "${output.glue-database.name}"
              table_name: "${output.glue-table.name}"
              role_arn: "${output.iam.firehose_role_arn}"
              region: "${region}"

          # Processing configuration
          processing_configuration:
            enabled: true
            processors:
              - type: "Lambda"
                parameters:
                  - parameter_name: "LambdaArn"
                    parameter_value: "${output.lambda-firehose-transformer.transform_arn}"
                  - parameter_name: "BufferSizeInMBs"
                    parameter_value: "1"
                  - parameter_name: "BufferIntervalInSeconds"
                    parameter_value: "60"
              - type: "MetadataExtraction"
                parameters:
                  - parameter_name: "MetadataExtractionQuery"
                    parameter_value: "{year:.timestamp | strftime(\"%Y\"), month:.timestamp | strftime(\"%m\"), day:.timestamp | strftime(\"%d\"), hour:.timestamp | strftime(\"%H\")}"
                  - parameter_name: "JsonParsingEngine"
                    parameter_value: "JQ-1.6"
              - type: "AppendDelimiterToRecord"
                parameters:
                  - parameter_name: "Delimiter"
                    parameter_value: "\\n"

          # CloudWatch logging
          cloudwatch_logging_options:
            enabled: true
            log_group_name: "${output.cloudwatch-logs.firehose_log_group_name}"
            log_stream_name: "s3-delivery"

          # S3 backup (all data)
          s3_backup_mode: "Enabled"
          s3_backup_configuration:
            bucket_arn: "${output.s3-backup.bucket_arn}"
            role_arn: "${output.iam.firehose_role_arn}"
            prefix: "backup/"
            buffering_size: 64
            buffering_interval: 300
            compression_format: "GZIP"

      tags:
        StackType: "streaming-pipeline"
        Layer: "delivery"

    firehose-opensearch:
      component: "firehose"
      depends_on:
        - kinesis-enriched
        - opensearch
      vars:
        enabled: "${enable_opensearch}"

        delivery_stream_name: "${tenant}-${environment}-opensearch-delivery"
        delivery_stream_type: "KinesisStreamAsSource"

        kinesis_source_configuration:
          kinesis_stream_arn: "${output.kinesis-enriched.stream_arn}"
          role_arn: "${output.iam.firehose_role_arn}"

        opensearch_configuration:
          domain_arn: "${output.opensearch.domain_arn}"
          role_arn: "${output.iam.firehose_role_arn}"
          index_name: "${tenant}-${environment}-events"
          index_rotation_period: "OneDay"
          type_name: ""  # Empty for OpenSearch

          buffering_size: 5  # MB
          buffering_interval: 60  # seconds

          retry_duration: 300

          s3_backup_mode: "FailedDocumentsOnly"

          cloudwatch_logging_options:
            enabled: true
            log_group_name: "${output.cloudwatch-logs.firehose_log_group_name}"
            log_stream_name: "opensearch-delivery"

          vpc_configuration:
            security_group_ids:
              - "${output.securitygroup.firehose_sg_id}"
            subnet_ids: "${output.vpc.private_subnet_ids}"
            role_arn: "${output.iam.firehose_role_arn}"

      tags:
        StackType: "streaming-pipeline"
        Layer: "delivery"

    firehose-redshift:
      component: "firehose"
      depends_on:
        - kinesis-enriched
        - s3-staging
      vars:
        enabled: "${enable_redshift}"

        delivery_stream_name: "${tenant}-${environment}-redshift-delivery"
        delivery_stream_type: "KinesisStreamAsSource"

        kinesis_source_configuration:
          kinesis_stream_arn: "${output.kinesis-enriched.stream_arn}"
          role_arn: "${output.iam.firehose_role_arn}"

        redshift_configuration:
          cluster_jdbcurl: "${output.redshift.jdbc_url}"
          username: "${output.secretsmanager.redshift_username}"
          password: "${output.secretsmanager.redshift_password}"
          role_arn: "${output.iam.firehose_role_arn}"
          data_table_name: "streaming_events"
          copy_options: "FORMAT AS JSON 'auto' GZIP TIMEFORMAT 'auto'"

          s3_configuration:
            bucket_arn: "${output.s3-staging.bucket_arn}"
            role_arn: "${output.iam.firehose_role_arn}"
            prefix: "redshift-staging/"
            buffering_size: 128
            buffering_interval: 300
            compression_format: "GZIP"

          s3_backup_mode: "Enabled"
          s3_backup_configuration:
            bucket_arn: "${output.s3-backup.bucket_arn}"
            role_arn: "${output.iam.firehose_role_arn}"
            prefix: "redshift-backup/"

          cloudwatch_logging_options:
            enabled: true
            log_group_name: "${output.cloudwatch-logs.firehose_log_group_name}"
            log_stream_name: "redshift-delivery"

      tags:
        StackType: "streaming-pipeline"
        Layer: "delivery"

    lambda-firehose-transformer:
      component: "lambda"
      vars:
        enabled: true

        functions:
          transform:
            function_name: "${tenant}-${environment}-firehose-transformer"
            description: "Transform records for Firehose delivery"
            runtime: "python3.11"
            handler: "transformers/firehose.handler"
            memory_size: 512
            timeout: 60
            architecture: "arm64"

            environment_variables:
              ENVIRONMENT: "${environment}"

            tracing_config:
              mode: "Active"

      tags:
        StackType: "streaming-pipeline"
        Layer: "delivery"

    # =========================================================================
    # STORAGE LAYER
    # =========================================================================
    s3-data-lake:
      component: "s3"
      vars:
        enabled: true
        bucket_name: "${tenant}-${environment}-data-lake"

        versioning_enabled: false
        sse_algorithm: "aws:kms"
        kms_master_key_id: "${output.kms.key_arn}"

        lifecycle_rules:
          - id: "intelligent-tiering"
            enabled: true
            transitions:
              - days: 0
                storage_class: "INTELLIGENT_TIERING"
          - id: "glacier-archive"
            enabled: true
            prefix: "archive/"
            transitions:
              - days: 90
                storage_class: "GLACIER"
            expiration:
              days: "${archive_retention_days}"

        intelligent_tiering_configuration:
          - name: "archive-tier"
            status: "Enabled"
            tiering:
              - access_tier: "ARCHIVE_ACCESS"
                days: 90
              - access_tier: "DEEP_ARCHIVE_ACCESS"
                days: 180

        block_public_access: true

        # Enable S3 Analytics
        analytics_configuration:
          - id: "data-lake-analytics"
            storage_class_analysis:
              data_export:
                destination:
                  s3_bucket_destination:
                    bucket_arn: "${output.s3-analytics.bucket_arn}"
                    prefix: "analytics/"

      tags:
        StackType: "streaming-pipeline"
        Layer: "storage"

    s3-backup:
      component: "s3"
      vars:
        enabled: true
        bucket_name: "${tenant}-${environment}-stream-backup"
        versioning_enabled: true
        sse_algorithm: "aws:kms"
        kms_master_key_id: "${output.kms.key_arn}"

        lifecycle_rules:
          - id: "backup-lifecycle"
            enabled: true
            transitions:
              - days: 30
                storage_class: "STANDARD_IA"
              - days: 90
                storage_class: "GLACIER"
            expiration:
              days: 365

        block_public_access: true

      tags:
        StackType: "streaming-pipeline"
        Layer: "storage"

    s3-staging:
      component: "s3"
      vars:
        enabled: true
        bucket_name: "${tenant}-${environment}-stream-staging"
        sse_algorithm: "aws:kms"
        kms_master_key_id: "${output.kms.key_arn}"

        lifecycle_rules:
          - id: "cleanup-staging"
            enabled: true
            expiration:
              days: 7

        block_public_access: true

      tags:
        StackType: "streaming-pipeline"
        Layer: "storage"

    s3-code:
      component: "s3"
      vars:
        enabled: true
        bucket_name: "${tenant}-${environment}-stream-code"
        versioning_enabled: true
        sse_algorithm: "aws:kms"
        kms_master_key_id: "${output.kms.key_arn}"
        block_public_access: true

      tags:
        StackType: "streaming-pipeline"
        Layer: "code"

    s3-analytics:
      component: "s3"
      vars:
        enabled: true
        bucket_name: "${tenant}-${environment}-s3-analytics"
        sse_algorithm: "AES256"
        block_public_access: true

        lifecycle_rules:
          - id: "cleanup"
            enabled: true
            expiration:
              days: 90

      tags:
        StackType: "streaming-pipeline"
        Layer: "analytics"

    opensearch:
      component: "opensearch"
      vars:
        enabled: "${enable_opensearch}"

        domain_name: "${tenant}-${environment}-analytics"
        engine_version: "OpenSearch_2.11"

        cluster_config:
          instance_type: "r6g.large.search"
          instance_count: 2
          dedicated_master_enabled: false
          zone_awareness_enabled: true
          zone_awareness_config:
            availability_zone_count: 2
          warm_enabled: false

        ebs_options:
          ebs_enabled: true
          volume_type: "gp3"
          volume_size: 100
          iops: 3000
          throughput: 125

        encrypt_at_rest:
          enabled: true
          kms_key_id: "${output.kms.key_arn}"

        node_to_node_encryption:
          enabled: true

        domain_endpoint_options:
          enforce_https: true
          tls_security_policy: "Policy-Min-TLS-1-2-2019-07"

        advanced_security_options:
          enabled: true
          internal_user_database_enabled: true
          master_user_options:
            master_user_name: "admin"
            master_user_password: "${output.secretsmanager.opensearch_password}"

        vpc_options:
          security_group_ids:
            - "${output.securitygroup.opensearch_sg_id}"
          subnet_ids:
            - "${output.vpc.private_subnet_ids[0]}"
            - "${output.vpc.private_subnet_ids[1]}"

        auto_tune_options:
          desired_state: "ENABLED"
          rollback_on_disable: "NO_ROLLBACK"
          maintenance_schedule:
            - start_at: "2024-01-01T00:00:00Z"
              duration:
                value: 2
                unit: "HOURS"
              cron_expression_for_recurrence: "cron(0 0 ? * SUN *)"

        log_publishing_options:
          INDEX_SLOW_LOGS:
            cloudwatch_log_group_arn: "${output.cloudwatch-logs.opensearch_index_log_group_arn}"
            enabled: true
          SEARCH_SLOW_LOGS:
            cloudwatch_log_group_arn: "${output.cloudwatch-logs.opensearch_search_log_group_arn}"
            enabled: true
          ES_APPLICATION_LOGS:
            cloudwatch_log_group_arn: "${output.cloudwatch-logs.opensearch_app_log_group_arn}"
            enabled: true

      tags:
        StackType: "streaming-pipeline"
        Layer: "storage"

    # =========================================================================
    # STATE MANAGEMENT
    # =========================================================================
    dynamodb-lookup:
      component: "dynamodb"
      vars:
        enabled: true
        table_name: "${tenant}-${environment}-stream-lookup"
        billing_mode: "PAY_PER_REQUEST"

        hash_key: "pk"

        attributes:
          - name: "pk"
            type: "S"

        point_in_time_recovery_enabled: true
        server_side_encryption_enabled: true
        server_side_encryption_kms_key_arn: "${output.kms.key_arn}"

        # Enable DAX for low-latency lookups
        dax_enabled: true
        dax_cluster_name: "${tenant}-${environment}-lookup-cache"
        dax_node_type: "dax.r5.large"
        dax_replication_factor: 2
        dax_subnet_group_name: "${output.vpc.dax_subnet_group_name}"
        dax_security_group_ids:
          - "${output.securitygroup.dax_sg_id}"

      tags:
        StackType: "streaming-pipeline"
        Layer: "state"

    dynamodb-state:
      component: "dynamodb"
      vars:
        enabled: true
        table_name: "${tenant}-${environment}-stream-state"
        billing_mode: "PAY_PER_REQUEST"

        hash_key: "pk"
        range_key: "sk"

        attributes:
          - name: "pk"
            type: "S"
          - name: "sk"
            type: "S"

        ttl_enabled: true
        ttl_attribute_name: "ttl"

        point_in_time_recovery_enabled: true
        server_side_encryption_enabled: true
        server_side_encryption_kms_key_arn: "${output.kms.key_arn}"

      tags:
        StackType: "streaming-pipeline"
        Layer: "state"

    elasticache:
      component: "elasticache"
      vars:
        enabled: true

        cluster_id: "${tenant}-${environment}-stream-cache"
        engine: "redis"
        engine_version: "7.0"
        node_type: "cache.r6g.large"
        num_cache_nodes: 2

        parameter_group_name: "default.redis7"

        automatic_failover_enabled: true
        multi_az_enabled: true

        at_rest_encryption_enabled: true
        transit_encryption_enabled: true
        auth_token: "${output.secretsmanager.redis_token}"

        subnet_group_name: "${output.vpc.elasticache_subnet_group_name}"
        security_group_ids:
          - "${output.securitygroup.elasticache_sg_id}"

        snapshot_retention_limit: 7
        snapshot_window: "03:00-05:00"
        maintenance_window: "sun:05:00-sun:07:00"

      tags:
        StackType: "streaming-pipeline"
        Layer: "caching"

    # =========================================================================
    # ERROR HANDLING
    # =========================================================================
    sqs-dlq:
      component: "sqs"
      vars:
        enabled: true
        queue_name: "${tenant}-${environment}-stream-dlq"

        message_retention_seconds: 1209600  # 14 days
        visibility_timeout_seconds: 300

        kms_master_key_id: "${output.kms.key_arn}"

      tags:
        StackType: "streaming-pipeline"
        Layer: "error-handling"

    sns-alerts:
      component: "sns"
      vars:
        enabled: true
        topic_name: "${tenant}-${environment}-stream-alerts"
        display_name: "Streaming Pipeline Alerts"
        kms_master_key_id: "${output.kms.key_arn}"

        subscriptions:
          - protocol: "email"
            endpoint: "streaming-alerts@${domain}"

      tags:
        StackType: "streaming-pipeline"
        Layer: "alerting"

    # =========================================================================
    # DATA CATALOG
    # =========================================================================
    glue-database:
      component: "glue"
      vars:
        enabled: true

        database_name: "${tenant}_${environment}_streaming"
        description: "Glue database for streaming data catalog"

        catalog_id: "${aws_account_id}"

      tags:
        StackType: "streaming-pipeline"
        Layer: "catalog"

    glue-table:
      component: "glue-table"
      depends_on:
        - glue-database
        - s3-data-lake
      vars:
        enabled: true

        database_name: "${output.glue-database.name}"
        table_name: "streaming_events"
        description: "Table for streaming event data"

        table_type: "EXTERNAL_TABLE"

        parameters:
          classification: "parquet"
          compressionType: "gzip"
          typeOfData: "file"

        storage_descriptor:
          location: "s3://${output.s3-data-lake.bucket_name}/data/"
          input_format: "org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat"
          output_format: "org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat"
          ser_de_info:
            serialization_library: "org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe"
          columns:
            - name: "event_id"
              type: "string"
            - name: "event_type"
              type: "string"
            - name: "timestamp"
              type: "timestamp"
            - name: "payload"
              type: "string"
            - name: "source"
              type: "string"
            - name: "partition_key"
              type: "string"

        partition_keys:
          - name: "year"
            type: "string"
          - name: "month"
            type: "string"
          - name: "day"
            type: "string"
          - name: "hour"
            type: "string"

      tags:
        StackType: "streaming-pipeline"
        Layer: "catalog"

    # =========================================================================
    # OBSERVABILITY
    # =========================================================================
    cloudwatch-logs:
      component: "cloudwatch-logs"
      vars:
        enabled: true

        log_groups:
          api:
            name: "/aws/apigateway/${tenant}-${environment}-stream-ingest"
            retention_in_days: "${log_retention_days}"
          lambda:
            name: "/aws/lambda/${tenant}-${environment}/stream-processors"
            retention_in_days: "${log_retention_days}"
          analytics:
            name: "/aws/kinesis-analytics/${tenant}-${environment}"
            retention_in_days: "${log_retention_days}"
          firehose:
            name: "/aws/kinesisfirehose/${tenant}-${environment}"
            retention_in_days: "${log_retention_days}"
          opensearch-index:
            name: "/aws/opensearch/${tenant}-${environment}/index-slow-logs"
            retention_in_days: "${log_retention_days}"
          opensearch-search:
            name: "/aws/opensearch/${tenant}-${environment}/search-slow-logs"
            retention_in_days: "${log_retention_days}"
          opensearch-app:
            name: "/aws/opensearch/${tenant}-${environment}/application-logs"
            retention_in_days: "${log_retention_days}"

      tags:
        StackType: "streaming-pipeline"
        Layer: "observability"

    monitoring:
      component: "monitoring"
      depends_on:
        - kinesis-ingest
        - kinesis-enriched
        - lambda-stream-processor
        - firehose-s3
      vars:
        enabled: true

        create_dashboard: true
        dashboard_name: "${tenant}-${environment}-streaming-pipeline"

        dashboard_widgets:
          - type: "metric"
            title: "Kinesis Ingest - Incoming Records"
            width: 12
            height: 6
            metrics:
              - namespace: "AWS/Kinesis"
                metric_name: "IncomingRecords"
                dimensions:
                  StreamName: "${output.kinesis-ingest.stream_name}"
                stat: "Sum"
                period: 60

          - type: "metric"
            title: "Kinesis Ingest - Iterator Age"
            width: 12
            height: 6
            metrics:
              - namespace: "AWS/Kinesis"
                metric_name: "GetRecords.IteratorAgeMilliseconds"
                dimensions:
                  StreamName: "${output.kinesis-ingest.stream_name}"
                stat: "Maximum"
                period: 60

          - type: "metric"
            title: "Lambda Processor - Invocations & Errors"
            width: 12
            height: 6
            metrics:
              - namespace: "AWS/Lambda"
                metric_name: "Invocations"
                dimensions:
                  FunctionName: "${output.lambda-stream-processor.stream_processor_name}"
                stat: "Sum"
                period: 60
              - namespace: "AWS/Lambda"
                metric_name: "Errors"
                dimensions:
                  FunctionName: "${output.lambda-stream-processor.stream_processor_name}"
                stat: "Sum"
                period: 60

          - type: "metric"
            title: "Lambda Processor - Duration"
            width: 12
            height: 6
            metrics:
              - namespace: "AWS/Lambda"
                metric_name: "Duration"
                dimensions:
                  FunctionName: "${output.lambda-stream-processor.stream_processor_name}"
                stat: "p99"
                period: 60
              - namespace: "AWS/Lambda"
                metric_name: "Duration"
                dimensions:
                  FunctionName: "${output.lambda-stream-processor.stream_processor_name}"
                stat: "Average"
                period: 60

          - type: "metric"
            title: "Firehose - Delivery Success"
            width: 12
            height: 6
            metrics:
              - namespace: "AWS/Firehose"
                metric_name: "DeliveryToS3.Success"
                dimensions:
                  DeliveryStreamName: "${output.firehose-s3.delivery_stream_name}"
                stat: "Sum"
                period: 60

          - type: "metric"
            title: "Firehose - Data Freshness"
            width: 12
            height: 6
            metrics:
              - namespace: "AWS/Firehose"
                metric_name: "DeliveryToS3.DataFreshness"
                dimensions:
                  DeliveryStreamName: "${output.firehose-s3.delivery_stream_name}"
                stat: "Maximum"
                period: 60

          - type: "metric"
            title: "DLQ - Message Count"
            width: 24
            height: 6
            metrics:
              - namespace: "AWS/SQS"
                metric_name: "ApproximateNumberOfMessagesVisible"
                dimensions:
                  QueueName: "${output.sqs-dlq.queue_name}"
                stat: "Sum"
                period: 60

        # Alarms
        alarm_notifications_enabled: true
        sns_topic_name: "${output.sns-alerts.topic_name}"

        alarms:
          # Kinesis Alarms
          kinesis-iterator-age:
            alarm_name: "${tenant}-${environment}-kinesis-iterator-age"
            metric_name: "GetRecords.IteratorAgeMilliseconds"
            namespace: "AWS/Kinesis"
            statistic: "Maximum"
            period: 300
            evaluation_periods: 3
            threshold: 300000  # 5 minutes
            comparison_operator: "GreaterThanThreshold"
            dimensions:
              StreamName: "${output.kinesis-ingest.stream_name}"
            alarm_description: "Kinesis iterator age exceeded 5 minutes - processing is falling behind"

          kinesis-write-throttled:
            alarm_name: "${tenant}-${environment}-kinesis-write-throttled"
            metric_name: "WriteProvisionedThroughputExceeded"
            namespace: "AWS/Kinesis"
            statistic: "Sum"
            period: 60
            evaluation_periods: 3
            threshold: 0
            comparison_operator: "GreaterThanThreshold"
            dimensions:
              StreamName: "${output.kinesis-ingest.stream_name}"
            alarm_description: "Kinesis write throttling detected"

          kinesis-read-throttled:
            alarm_name: "${tenant}-${environment}-kinesis-read-throttled"
            metric_name: "ReadProvisionedThroughputExceeded"
            namespace: "AWS/Kinesis"
            statistic: "Sum"
            period: 60
            evaluation_periods: 3
            threshold: 0
            comparison_operator: "GreaterThanThreshold"
            dimensions:
              StreamName: "${output.kinesis-ingest.stream_name}"
            alarm_description: "Kinesis read throttling detected"

          # Lambda Alarms
          lambda-processor-errors:
            alarm_name: "${tenant}-${environment}-lambda-processor-errors"
            metric_name: "Errors"
            namespace: "AWS/Lambda"
            statistic: "Sum"
            period: 300
            evaluation_periods: 2
            threshold: 10
            comparison_operator: "GreaterThanThreshold"
            dimensions:
              FunctionName: "${output.lambda-stream-processor.stream_processor_name}"
            alarm_description: "Lambda processor errors exceeded threshold"

          lambda-processor-duration:
            alarm_name: "${tenant}-${environment}-lambda-processor-duration"
            metric_name: "Duration"
            namespace: "AWS/Lambda"
            extended_statistic: "p99"
            period: 300
            evaluation_periods: 3
            threshold: 50000  # 50 seconds (near 60 second timeout)
            comparison_operator: "GreaterThanThreshold"
            dimensions:
              FunctionName: "${output.lambda-stream-processor.stream_processor_name}"
            alarm_description: "Lambda processor p99 duration approaching timeout"

          # Firehose Alarms
          firehose-delivery-success-rate:
            alarm_name: "${tenant}-${environment}-firehose-delivery"
            metric_name: "DeliveryToS3.Success"
            namespace: "AWS/Firehose"
            statistic: "Average"
            period: 300
            evaluation_periods: 3
            threshold: 0.95
            comparison_operator: "LessThanThreshold"
            dimensions:
              DeliveryStreamName: "${output.firehose-s3.delivery_stream_name}"
            alarm_description: "Firehose delivery success rate below 95%"

          firehose-data-freshness:
            alarm_name: "${tenant}-${environment}-firehose-freshness"
            metric_name: "DeliveryToS3.DataFreshness"
            namespace: "AWS/Firehose"
            statistic: "Maximum"
            period: 300
            evaluation_periods: 2
            threshold: 900  # 15 minutes
            comparison_operator: "GreaterThanThreshold"
            dimensions:
              DeliveryStreamName: "${output.firehose-s3.delivery_stream_name}"
            alarm_description: "Firehose data freshness exceeded 15 minutes"

          # DLQ Alarm
          dlq-messages:
            alarm_name: "${tenant}-${environment}-stream-dlq-messages"
            metric_name: "ApproximateNumberOfMessagesVisible"
            namespace: "AWS/SQS"
            statistic: "Sum"
            period: 60
            evaluation_periods: 1
            threshold: 1
            comparison_operator: "GreaterThanOrEqualToThreshold"
            dimensions:
              QueueName: "${output.sqs-dlq.queue_name}"
            alarm_description: "Messages present in streaming DLQ - requires attention"

      tags:
        StackType: "streaming-pipeline"
        Layer: "observability"

    # =========================================================================
    # IAM
    # =========================================================================
    iam:
      component: "iam"
      vars:
        enabled: true

        roles:
          # Lambda Execution Role
          lambda-stream:
            name: "${tenant}-${environment}-stream-lambda-role"
            assume_role_policy:
              Version: "2012-10-17"
              Statement:
                - Effect: "Allow"
                  Principal:
                    Service: "lambda.amazonaws.com"
                  Action: "sts:AssumeRole"

            managed_policy_arns:
              - "arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole"
              - "arn:aws:iam::aws:policy/service-role/AWSLambdaVPCAccessExecutionRole"
              - "arn:aws:iam::aws:policy/AWSXRayDaemonWriteAccess"

            inline_policies:
              kinesis:
                - Effect: "Allow"
                  Action:
                    - "kinesis:GetRecords"
                    - "kinesis:GetShardIterator"
                    - "kinesis:DescribeStream"
                    - "kinesis:DescribeStreamSummary"
                    - "kinesis:ListShards"
                    - "kinesis:ListStreams"
                    - "kinesis:PutRecord"
                    - "kinesis:PutRecords"
                    - "kinesis:SubscribeToShard"
                  Resource:
                    - "arn:aws:kinesis:${region}:${aws_account_id}:stream/${tenant}-${environment}-*"
              dynamodb:
                - Effect: "Allow"
                  Action:
                    - "dynamodb:GetItem"
                    - "dynamodb:PutItem"
                    - "dynamodb:UpdateItem"
                    - "dynamodb:Query"
                    - "dynamodb:BatchGetItem"
                  Resource:
                    - "arn:aws:dynamodb:${region}:${aws_account_id}:table/${tenant}-${environment}-*"
              sqs:
                - Effect: "Allow"
                  Action:
                    - "sqs:SendMessage"
                    - "sqs:ReceiveMessage"
                    - "sqs:DeleteMessage"
                    - "sqs:GetQueueAttributes"
                  Resource:
                    - "arn:aws:sqs:${region}:${aws_account_id}:${tenant}-${environment}-*"
              sns:
                - Effect: "Allow"
                  Action:
                    - "sns:Publish"
                  Resource:
                    - "arn:aws:sns:${region}:${aws_account_id}:${tenant}-${environment}-*"
              elasticache:
                - Effect: "Allow"
                  Action:
                    - "elasticache:Connect"
                  Resource:
                    - "arn:aws:elasticache:${region}:${aws_account_id}:replicationgroup:${tenant}-${environment}-*"
              kms:
                - Effect: "Allow"
                  Action:
                    - "kms:Decrypt"
                    - "kms:GenerateDataKey"
                  Resource:
                    - "${output.kms.key_arn}"

          # API Gateway to Kinesis Role
          api-kinesis:
            name: "${tenant}-${environment}-api-kinesis-role"
            assume_role_policy:
              Version: "2012-10-17"
              Statement:
                - Effect: "Allow"
                  Principal:
                    Service: "apigateway.amazonaws.com"
                  Action: "sts:AssumeRole"

            inline_policies:
              kinesis:
                - Effect: "Allow"
                  Action:
                    - "kinesis:PutRecord"
                    - "kinesis:PutRecords"
                  Resource:
                    - "${output.kinesis-ingest.stream_arn}"

          # Firehose Role
          firehose:
            name: "${tenant}-${environment}-firehose-role"
            assume_role_policy:
              Version: "2012-10-17"
              Statement:
                - Effect: "Allow"
                  Principal:
                    Service: "firehose.amazonaws.com"
                  Action: "sts:AssumeRole"

            inline_policies:
              kinesis:
                - Effect: "Allow"
                  Action:
                    - "kinesis:DescribeStream"
                    - "kinesis:GetShardIterator"
                    - "kinesis:GetRecords"
                    - "kinesis:DescribeStreamSummary"
                  Resource:
                    - "arn:aws:kinesis:${region}:${aws_account_id}:stream/${tenant}-${environment}-*"
              s3:
                - Effect: "Allow"
                  Action:
                    - "s3:AbortMultipartUpload"
                    - "s3:GetBucketLocation"
                    - "s3:GetObject"
                    - "s3:ListBucket"
                    - "s3:ListBucketMultipartUploads"
                    - "s3:PutObject"
                  Resource:
                    - "arn:aws:s3:::${tenant}-${environment}-*"
                    - "arn:aws:s3:::${tenant}-${environment}-*/*"
              glue:
                - Effect: "Allow"
                  Action:
                    - "glue:GetTable"
                    - "glue:GetTableVersion"
                    - "glue:GetTableVersions"
                  Resource:
                    - "arn:aws:glue:${region}:${aws_account_id}:catalog"
                    - "arn:aws:glue:${region}:${aws_account_id}:database/${tenant}_${environment}_streaming"
                    - "arn:aws:glue:${region}:${aws_account_id}:table/${tenant}_${environment}_streaming/*"
              lambda:
                - Effect: "Allow"
                  Action:
                    - "lambda:InvokeFunction"
                    - "lambda:GetFunctionConfiguration"
                  Resource:
                    - "arn:aws:lambda:${region}:${aws_account_id}:function:${tenant}-${environment}-firehose-*"
              opensearch:
                - Effect: "Allow"
                  Action:
                    - "es:DescribeDomain"
                    - "es:DescribeDomains"
                    - "es:DescribeDomainConfig"
                    - "es:ESHttpPost"
                    - "es:ESHttpPut"
                  Resource:
                    - "arn:aws:es:${region}:${aws_account_id}:domain/${tenant}-${environment}-*"
                    - "arn:aws:es:${region}:${aws_account_id}:domain/${tenant}-${environment}-*/*"
              kms:
                - Effect: "Allow"
                  Action:
                    - "kms:Decrypt"
                    - "kms:GenerateDataKey"
                  Resource:
                    - "${output.kms.key_arn}"
              logs:
                - Effect: "Allow"
                  Action:
                    - "logs:PutLogEvents"
                  Resource:
                    - "arn:aws:logs:${region}:${aws_account_id}:log-group:/aws/kinesisfirehose/${tenant}-${environment}:log-stream:*"

          # Kinesis Analytics Role
          analytics:
            name: "${tenant}-${environment}-analytics-role"
            assume_role_policy:
              Version: "2012-10-17"
              Statement:
                - Effect: "Allow"
                  Principal:
                    Service: "kinesisanalytics.amazonaws.com"
                  Action: "sts:AssumeRole"

            inline_policies:
              kinesis:
                - Effect: "Allow"
                  Action:
                    - "kinesis:DescribeStream"
                    - "kinesis:GetShardIterator"
                    - "kinesis:GetRecords"
                    - "kinesis:DescribeStreamSummary"
                    - "kinesis:PutRecord"
                    - "kinesis:PutRecords"
                  Resource:
                    - "arn:aws:kinesis:${region}:${aws_account_id}:stream/${tenant}-${environment}-*"
              s3:
                - Effect: "Allow"
                  Action:
                    - "s3:GetObject"
                    - "s3:GetObjectVersion"
                  Resource:
                    - "${output.s3-code.bucket_arn}/*"
              logs:
                - Effect: "Allow"
                  Action:
                    - "logs:DescribeLogGroups"
                    - "logs:DescribeLogStreams"
                    - "logs:PutLogEvents"
                  Resource:
                    - "arn:aws:logs:${region}:${aws_account_id}:log-group:/aws/kinesis-analytics/${tenant}-${environment}:*"
              vpc:
                - Effect: "Allow"
                  Action:
                    - "ec2:DescribeVpcs"
                    - "ec2:DescribeSubnets"
                    - "ec2:DescribeSecurityGroups"
                    - "ec2:DescribeDhcpOptions"
                    - "ec2:CreateNetworkInterface"
                    - "ec2:CreateNetworkInterfacePermission"
                    - "ec2:DeleteNetworkInterface"
                    - "ec2:DescribeNetworkInterfaces"
                  Resource: "*"

      tags:
        StackType: "streaming-pipeline"
        Layer: "security"

    # =========================================================================
    # ENCRYPTION
    # =========================================================================
    kms:
      component: "kms"
      vars:
        enabled: true
        alias: "alias/${tenant}-${environment}-streaming"
        description: "KMS key for streaming pipeline encryption"
        deletion_window_in_days: 30
        enable_key_rotation: true

        key_policy:
          Version: "2012-10-17"
          Statement:
            - Sid: "Enable IAM User Permissions"
              Effect: "Allow"
              Principal:
                AWS: "arn:aws:iam::${aws_account_id}:root"
              Action: "kms:*"
              Resource: "*"
            - Sid: "Allow Kinesis"
              Effect: "Allow"
              Principal:
                Service: "kinesis.amazonaws.com"
              Action:
                - "kms:Decrypt"
                - "kms:GenerateDataKey"
              Resource: "*"
            - Sid: "Allow Firehose"
              Effect: "Allow"
              Principal:
                Service: "firehose.amazonaws.com"
              Action:
                - "kms:Decrypt"
                - "kms:GenerateDataKey"
              Resource: "*"

      tags:
        StackType: "streaming-pipeline"
        Layer: "security"

    # =========================================================================
    # SECRETS
    # =========================================================================
    secretsmanager:
      component: "secretsmanager"
      vars:
        enabled: true

        secrets:
          opensearch:
            name: "${tenant}-${environment}/streaming/opensearch"
            description: "OpenSearch admin credentials"
            generate_random_password: true
            password_length: 32
          redis:
            name: "${tenant}-${environment}/streaming/redis"
            description: "Redis AUTH token"
            generate_random_password: true
            password_length: 64

      tags:
        StackType: "streaming-pipeline"
        Layer: "security"

# =============================================================================
# DEPLOYMENT CONFIGURATION
# =============================================================================
deployment_order:
  - stage: 1
    name: "Foundation"
    components: [kms, iam, secretsmanager, cloudwatch-logs]
    parallel: true
    description: "Deploy security and logging foundation"

  - stage: 2
    name: "Networking"
    components: [vpc, securitygroup]
    parallel: false
    description: "Deploy VPC and security groups"

  - stage: 3
    name: "Storage"
    components: [s3-data-lake, s3-backup, s3-staging, s3-code, s3-analytics, sqs-dlq]
    parallel: true
    description: "Deploy storage infrastructure"

  - stage: 4
    name: "State Management"
    components: [dynamodb-lookup, dynamodb-state, elasticache]
    parallel: true
    description: "Deploy state management"

  - stage: 5
    name: "Data Catalog"
    components: [glue-database, glue-table]
    parallel: false
    description: "Deploy Glue data catalog"

  - stage: 6
    name: "Ingestion"
    components: [kinesis-ingest, kinesis-enriched, api-ingest]
    parallel: true
    description: "Deploy data ingestion streams"

  - stage: 7
    name: "Analytics Storage"
    components: [opensearch]
    parallel: false
    description: "Deploy OpenSearch domain"

  - stage: 8
    name: "Processing"
    components: [lambda-stream-processor, lambda-firehose-transformer]
    parallel: true
    description: "Deploy stream processors"

  - stage: 9
    name: "Analytics"
    components: [kinesis-analytics]
    parallel: false
    description: "Deploy Kinesis Data Analytics"

  - stage: 10
    name: "Delivery"
    components: [firehose-s3, firehose-opensearch]
    parallel: true
    description: "Deploy Firehose delivery streams"

  - stage: 11
    name: "Alerting"
    components: [sns-alerts]
    parallel: false
    description: "Deploy alerting"

  - stage: 12
    name: "Observability"
    components: [monitoring]
    parallel: false
    description: "Deploy monitoring dashboards and alarms"

# =============================================================================
# VALIDATION RULES
# =============================================================================
validation:
  pre_deployment:
    - check: "iam_permissions"
      description: "Verify IAM permissions for all services"
    - check: "kms_key_exists"
      description: "Verify KMS key availability"
    - check: "vpc_available"
      description: "Verify VPC and subnets exist"
    - check: "lambda_code_exists"
      description: "Verify Lambda code packages exist"
    - check: "flink_jar_exists"
      description: "Verify Flink application JAR exists in S3"

  post_deployment:
    - check: "kinesis_stream_active"
      description: "Verify Kinesis streams are ACTIVE"
    - check: "lambda_functions_healthy"
      description: "Verify Lambda functions are invokable"
    - check: "firehose_delivery_active"
      description: "Verify Firehose delivery streams are active"
    - check: "opensearch_cluster_healthy"
      description: "Verify OpenSearch cluster is healthy"
    - check: "dlq_empty"
      description: "Verify DLQ is empty"

# =============================================================================
# TESTING STRATEGY
# =============================================================================
testing:
  unit_tests:
    - name: "Lambda processor tests"
      path: "tests/unit/processors/"
      command: "pytest tests/unit/processors/ -v"

    - name: "Transformer tests"
      path: "tests/unit/transformers/"
      command: "pytest tests/unit/transformers/ -v"

  integration_tests:
    - name: "End-to-end pipeline test"
      description: "Send test records through complete pipeline"
      command: |
        aws kinesis put-record \
          --stream-name ${tenant}-${environment}-ingest-stream \
          --data "$(echo '{"test": "data"}' | base64)" \
          --partition-key "test"

    - name: "Verify S3 delivery"
      description: "Check data arrived in S3"
      command: "tests/integration/verify_s3_delivery.sh"

    - name: "Verify OpenSearch delivery"
      description: "Check data arrived in OpenSearch"
      command: "tests/integration/verify_opensearch_delivery.sh"

  load_tests:
    - name: "Throughput test"
      description: "Test pipeline throughput"
      tool: "kinesis-data-generator"
      config: "tests/load/kinesis-throughput.json"

# =============================================================================
# ENVIRONMENT OVERRIDES
# =============================================================================
environment_overrides:
  development:
    vars:
      enable_enhanced_fanout: false
      enable_analytics: false
      enable_opensearch: false
      data_retention_hours: 24
      log_retention_days: 7
    components:
      kinesis-ingest:
        vars:
          stream_mode: "PROVISIONED"
          shard_count: 1
      lambda-stream-processor:
        vars:
          functions:
            stream-processor:
              memory_size: 512
              reserved_concurrent_executions: 10
      elasticache:
        vars:
          node_type: "cache.t3.micro"
          num_cache_nodes: 1
          automatic_failover_enabled: false

  staging:
    vars:
      enable_enhanced_fanout: true
      enable_analytics: false
      enable_opensearch: true
      data_retention_hours: 72
      log_retention_days: 14
    components:
      kinesis-ingest:
        vars:
          stream_mode: "ON_DEMAND"
      opensearch:
        vars:
          cluster_config:
            instance_type: "r6g.large.search"
            instance_count: 2

  production:
    vars:
      enable_enhanced_fanout: true
      enable_analytics: true
      enable_opensearch: true
      data_retention_hours: 168
      log_retention_days: 90
      archive_retention_days: 2555  # 7 years
    components:
      kinesis-ingest:
        vars:
          stream_mode: "ON_DEMAND"
      lambda-stream-processor:
        vars:
          functions:
            stream-processor:
              memory_size: 2048
              reserved_concurrent_executions: 500
              provisioned_concurrency: 10
      opensearch:
        vars:
          cluster_config:
            instance_type: "r6g.xlarge.search"
            instance_count: 3
            dedicated_master_enabled: true
            dedicated_master_type: "r6g.large.search"
            dedicated_master_count: 3
      elasticache:
        vars:
          node_type: "cache.r6g.xlarge"
          num_cache_nodes: 3
